\section{Project Idea}
\subsection{Scope/Topic} 
We look into [file hosting services (Dropbox) | revision systems (Github)], which is a popular system for [ collaborative work, remote file storage, FILL IN ].
\subsection{Problem} 
These services/systems work well with centralized storage, but keeping all data at a central server or group of servers:
\begin{enumerate}
\item Increases risk of privacy issues
\item Central point of failure
\end{enumerate}
[FILL IN more from SOUP and elsewhere]
\subsection{Solution}
Distribute data storage for these [file hosting services | revision systems] across all users, by storing replicas on individual user machines.

-Punt on security: SHA and PKI and everything else those schizoid nerds have come up with

To distribute data storage in such a manner requires a distributed and scalable replication strategy.
We investigate three common replication strategies:
\begin{itemize}
\item \textbf{Read Once, Write All (ROWA):} ROWA systems allow read operations to merely fetch the requested data from the most convenient location, while write operations are required to update all replicas. 
Read operations are therefore guaranteed to be correct, but if replica-storing node fails during a write operation, then that write operation will fail. (PE: necessarily true?)
\item \textbf{Read Once, Write All-Available (ROWA-A):} ROWA-A read operations are the same as in ROWA, but write operations only affect available replicas. 
This allows writes to subsets of nodes and better handles changes in node availability, but could compromise the correctness of subsequent read requests.
\item \textbf{Quorum Based (QB):} QB systems allow write operations to subsets of nodes without compromising correctness and consistency. An acting node must collect votes from other nodes until the sum of those votes exceeds predefined read or write quorums.  Read and write quorums are defined such that there is always a non-null intersection between any two quorum sets.
\end{itemize}

\subsection{Significance}
To the best of our knowledge, these strategies have not been comparatively evaluated with respect to distributed [file hosting | revision ] systems.

\subsection{Questions}
(Given that nodes do not know global state----and are dealing with a dynamic distributed system--nodes may come and go, nodes may crash, multiple users may work on the same data)
\begin{enumerate}
\item Timing/Synchronicity:

-How do we handle distributed updates of replicas, or rather how do we synchronize updates?

-How do we synchronize updates

\item Availability vs Network Overhead:

-How do we handle node failures?  How do we update data when a node returns?

-How do we ensure that a node gets updated/current data?

\item Group membership:

-How do we keep track of which nodes have replicas?
\end{enumerate}

\section{Related Work}
(synchronicity/replication strategies--conservative or optimistic--in DS, issues with DropBox, open source revision|file hosting)
(FILL IN)
(some good work on security issues of Dropbox)

\section{Design}
\subsection{Client}
Clients are thin--they have local data storage and the broker program, which handles their read and write requests.
We implement clients as individual processes.
\subsection{Broker}
The purpose of the broker in our scheme is to handle read and write requests of the individual clients. 
The broker keeps record of all clients participating in the system.
When it receives a read request, it pulls in the most current data from the entire system that it can find
and delivers it to the requesting client.
During a write request, the broker pushes the update to all the clients it can reach.
\subsection{Messages}
-Write (create, push update)
-Read (pull update)
-Join (group, project|revision ?)
Version numbers?
\subsection{Replication Strategies}
SEE ABOVE
\subsection{Network}
Clients and brokers use TCP stream sockets to communicate with one another.
\subsection{NOTES}
Client joins network, and sets up a broker. The client then immediately does a pull (to get all updated repo info) and also pushes any necessary new local data to repo.
Once update process is successful, client then listens to the broker and listens to its local repository.

\section{Evaluation}
\begin{enumerate}
\item Testing Platform:
\item Parameters:
Centralized broker vs decentralized broker.

Different levels of node churn.

Different replication strategies.
\item Metrics:
\begin{itemize}
\item Available:

-network overhead?

-failed pull attempts?
\item Reliable:

-number of successful up-to-date pulls?
\end{itemize}
\item Scalability:
\end{enumerate}
\section{Timeline}
\begin{enumerate}
\item This weekend: complete literature review (synchronicity/replication strategies in DS, issues with DropBox, open source revision|file hosting)

-Each of us: research 1 of each

-Fill in 1 paragraph summary of any literature, add any ideas to notes

\item Week 8: Design
  \begin{enumerate}
    \item Build client
    \item Implement storage
    \item Create network definition (graph, edges are cost)
    \item Implement central broker (define policies)
    \item Implement Distributed broker
    \item WHERE ARE WE?
  \end{enumerate}
\item Week 9 (Thanksgiving): Testing/Evaluation
\item Week 10: Presentation, Write paper
\item Final Exam Week: Write paper, turn in paper, have multiple celebratory beers
\end{enumerate}
